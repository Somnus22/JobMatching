{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import sys\n",
    "import subprocess\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import pymupdf\n",
    "import io\n",
    "import csv\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "skill_dictionaries = {\n",
    "    \"Programming Language (PL)\" : [\n",
    "        \"python\",\"java\",'javascript','java script',\"c++\",\"c\",\"c#\",\"ruby\",'go','php','swift','typescript','type script','kotlin',\n",
    "        'rust','scala','perl','r','matlab','assembly','visual basic','visual basic for applications','vba'\n",
    "    ],\n",
    "\n",
    "    \"Framework (FW)\": [\n",
    "        \"django\", \"flask\", \"spring\", \"spring framework\", \"spring boot\", \"react\", \"reactnative\", \"angular\", \"vue\", \"express\", \"fastapi\", \"asp.net\", \n",
    "        \".net\", \"laravel\", \"ruby on rails\", \"symfony\", \"meteor\", \"gatsby\", \"svelte\", \"phoenix\", \"cake\"\n",
    "    ],\n",
    "\n",
    "    \"Database (DB)\": [\n",
    "        \"datastore\", \"firestore\", \"metastore\", \"blob storage\", \"object storage\", \"file storage\", \"data storage\", \"disk storage\", \"cloud storage\",\n",
    "        \"database\", \"knowledgebase\", \"firebase\", \"hbase\", \"database management\", \"database system\", \"database administration\"\n",
    "        \"data warehouse\", \"data lake\", \"data mart\", \"data repository\", \"data center\", \"data server\", \"data modeling\",\n",
    "        \"redis\", \"cassandra\"\n",
    "    ],\n",
    "\n",
    "    \"Cloud Platform (CP)\":[\n",
    "        \"cloud\", \"cloud computing\", \"cloud storage\", \"cloud infrastructure\", \"cloud platform\",\n",
    "        \"azure\", \"azure kubernetes service\", \"microsoft azure\", \n",
    "        \"gcp\", \"google cloud\", \"google cloud platform\", \"google cloud functions\",\n",
    "        \"amazon web services\", \"ibm cloud\", \"oracle cloud\", \"digital ocean\", \"heroku\",\n",
    "        \"serverless\", \"iaas\", \"paas\", \"saas\", \"faas\", \"baas\", \"caas\",\n",
    "        \"s3\", \"ec2\", \"lambda\", \"elastic beanstalk\", \n",
    "        \"ecs\", \"eks\", \"fargate\", \"sqs\", \"sns\", \"rds\", \"redshift\"\n",
    "    ],\n",
    "\n",
    "    \"DevOps (DO)\":[\n",
    "        \"devops\", \"devsecops\",\n",
    "        \"ci\", \"ci/cd\", \"continuous integration\", \"continuous delivery\", \"continuous deployment\",\n",
    "        \"continuous integration/continuous delivery\", \"continuous integration/continuous deployment\",\n",
    "        \"version control\", \"git\", \"github\", \"gitlab\",       \n",
    "        \"jenkins\", \"travisci\", \"circleci\", \"teamci\", \"bamboo\",    \n",
    "        \"configuration management\", \"ansible\", \"puppet\", \"terraform\",   \n",
    "        \"container\", \"containerization\", \"containerisation\", \n",
    "        \"docker\", \"dockerization\", \"dockerized\", \"dockerisation\", \"dockerised\", \"docker swarm\", \"kubernetes\", \"helm\", \n",
    "        \"monitor\", \"monitoring\", \"prometheus\", \"grafana\",\n",
    "        \"unit testing\", \"integration testing\", \"selenium\", \"junit\", \"cypress\", \"jest\",\n",
    "        \"load balancing\", \"scale\", \"scaling\", \"scalability\", \"disaster recovery\", \"chaos engineering\"           \n",
    "    ],\n",
    "\n",
    "    \"Network & Security (NS)\":[\n",
    "        \"network\", \"network security\", \"network engineering\", \"network architecture\",\n",
    "        \"security\", \"security analyst\", \"security engineering\", \"security architecture\",\n",
    "        \"routing\", \"switching\", \"firewall\", \"firewall configuration\", \"vpn\", \"tcp\", \"tcp/ip\",\n",
    "        \"dns\", \"dhcp\", \"ccna\", \"ccna/ccnp\", \"sd-wan\", \"vlans\", \"subnet\",\n",
    "        \"risk\", \"incident response\", \"incident management\", \"identity and access management\",\n",
    "        \"iam\", \"iams\", \"mfa\", \"multi-factor authentication\", \"vulnerability assessment\",\n",
    "        \"vulnerability management\", \"vulnerability analysis\",\n",
    "        \"crowdstrike\", \"carbon black\", \"cisco\", \"defense\", \"sso\", \"oauth\", \"saml\", \"hsm\",\n",
    "        \"data encryption\", \"disk encryption\", \"owasp zap\", \"ethical hacking\", \"penetration testing\"\n",
    "    ],\n",
    "\n",
    "    \"Data Analysis & Science (DAS)\":[\n",
    "        \"pandas\", \"numpy\", \"scikit learn\", \"tensorflow\", \"power bi\", \"excel\", \"tableau\", \"matplotlib\",\n",
    "        \"data analytics\", \"data analysis\", \"data science\", \"data visualisation\", \"data visualization\",'dashboard', \"visualizing data\",\n",
    "        \"ai\", \"artificial intelligence\", \"machine learning\", \"ml\", \"nlp\", \"natural language processing\",\n",
    "        \"text analytics\", \"language model\", \"language models\", \"language modeling\",\n",
    "        \"transaction management\", \"data transaction\", \"data transactions\"\n",
    "    ],\n",
    "\n",
    "    \"Software Engineering (SWE)\":[\n",
    "        \"software development\", \"design patterns\", \"full stack\", \"fullstack\", \"full-stack\",\n",
    "        \"code optimization\", \"code optimisation\", \"performance tuning\", \"code refactoring\", \"refactoring\",\n",
    "        \"code review\", \"code reviews\", \"peer review\", \"peer reviews\", 'test-driven'\n",
    "        \"microservice\", \"microservices\", \"MSA\", \"microservice architecture\",\n",
    "        \"k8s\", \"eclipse\", \"eclipse ide\", \"hibernate\", \"hibernate orm\", \"jquery\",\n",
    "        \"rest api\", \"rest apis\", \"restful api\", \"restful apis\", \"restful web service\", \"restful web services\",\n",
    "        \"api development\", \"web service\", \"web services\",\n",
    "        \"object oriented programming\", \"oop\", \"object oriented design\", \n",
    "        \"apache\", \"apache kafka\", \"apache tomcat\", \"apache maven\", \"apache ant\", \n",
    "        \"apache struts\", \"apache camel\", \"apache spark\", \"apache hadoop\", \"apache flink\",\n",
    "        \"bootstrap\", \"front-end framework\", \"rabbitmq\", \"message queue\", \"message broker\",\n",
    "        \"front-end\", \"frontend\", \"back-end\", \"backend\", \"web design\", \"ui\", \"user interface\", \"ux\", \"user experience\", \"ui/ux\",\n",
    "        \"front-end development\", \"back-end development\", \"web development\"\n",
    "    ],\n",
    "\n",
    "    \"Project Management (PM)\":[\n",
    "        \"agile\", 'waterfall', 'atlassian', 'confluence', \"jira\", \"trello\", \"asana\", \"kanban\", \"prince2\", \"stakeholder management\", \"scrum\"\n",
    "    ],\n",
    "\n",
    "    \"Education Certification (EC)\":[\n",
    "        \"certified scrum master\", \"csm\", \"pmp\", \"aws certified\", \"azure certified\", \n",
    "        \"gcp certified\", \"cissp\", \"ccna\", \"ceh\", \"comptia\", \"cisa\", \"cism\",\n",
    "        \"bachelor of\", \"bachelor of science\", \"bachelor of engineering\", \"bachelor of computer science\",\n",
    "        \"bachelor of information technology\", \"bachelor of information systems\", \n",
    "        \"bachelor of cybersecurity\", \"bachelor of data science\", \"bachelor of software engineering\",\n",
    "        \"B.S.\", \"BS\", \"B.E.\", \"BE\", \"B.C.S.\", \"BCS\", \"B.Tech.\", \"BTech\",\n",
    "        \"Master's\",\"master of science\", \"master of engineering\", \"master of computer science\",\n",
    "        \"master of information technology\", \"master of information systems\",\n",
    "        \"master of cybersecurity\", \"master of data science\", \"master of software engineering\",\n",
    "        \"specialisation in software\", \"specialisation in data\", \"specialisation in cloud\",\n",
    "        \"specialisation in security\", \"specialisation in networking\", \"specialisation in ai\",\n",
    "        \"specialisation in machine learning\", \"specialization in software\", \"specialization in data\",\n",
    "        \"specialization in cloud\", \"specialization in security\", \"specialization in networking\",\n",
    "        \"specialization in ai\", \"specialization in machine learning\",\n",
    "        \"minor in computer science\", \"minor in information technology\", \"minor in data science\",\n",
    "        \"minor in software engineering\", \"major in computer science\", \"major in information technology\",\n",
    "        \"major in data science\", \"major in software engineering\", \n",
    "        \"information systems\",\"computer science\", \"computer engineering\"\n",
    "        \"bachelor's degree\", \"bachelor degree\", \"ba/bs\", \"bs/ba\", \"b.s.\", \"b.a.\", \"undergraduate degree\",\n",
    "        \"master's degree\",\n",
    "        \"phd\", \"doctorate\", \"doctoral degree\", \"ph.d.\",\n",
    "        \"computer science\", \"computer engineering\", \"software engineering\", \"information technology\", \n",
    "        \"information systems\", \"data science\", \"science\"\n",
    "    ],\n",
    "\n",
    "    \"Soft Skills (SS)\":[\n",
    "        \"communication\", \"communication skill\", \"communication skills\", \n",
    "        \"verbal communication\", \"written communication\",\n",
    "        \"leadership\", \"leadership skill\", \"leadership skills\", \n",
    "        \"decision making\", \"decision-making\",\n",
    "        \"team building\", \"team-building\",\n",
    "        \"mentoring\", \"coaching\",\n",
    "        \"strategic thinking\", \"strategic-thinking\",\n",
    "        \"teamwork\", \"collaboration\", \"team player\",\n",
    "        \"interpersonal\", \"interpersonal skill\", \"interpersonal skills\",\n",
    "        \"problem solving\", \"problem-solving\",\n",
    "        \"critical thinking\", \"critical-thinking\",\n",
    "        \"adaptability\", \"flexibility\", \"creativity\",\n",
    "        \"self motivated\", \"self-motivated\",\n",
    "        \"detail oriented\", \"detail-oriented\",\n",
    "        \"work ethic\",\n",
    "        \"time management\", \"time-management\",\n",
    "        \"organizational\", \"organizational skill\", \"organizational skills\",\n",
    "        \"multitasking\", \"prioritization\",\n",
    "        \"project coordination\",\n",
    "        \"conflict resolution\", \"conflict-resolution\",\n",
    "        \"emotional intelligence\",\n",
    "        \"presentation\", \"presentation skill\", \"presentation skills\",\n",
    "        \"public speaking\", \"public-speaking\",\n",
    "        \"negotiation\", \"negotiation skill\", \"negotiation skills\",\n",
    "        \"customer service\",\n",
    "        \"active listening\", \"active-listening\",\n",
    "        \"stress management\", \"stress-management\",\n",
    "        \"cultural awareness\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace('.', '')\n",
    "    return text\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    \"\"\"Preprocess text using spaCy's capabilities\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def extract_ngrams_spacy(doc, max_n=4):\n",
    "    \"\"\"Extract n-grams using spaCy's token attributes\"\"\"\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    \n",
    "    all_ngrams = []\n",
    "    for n in range(1, min(max_n + 1, len(tokens) + 1)):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            all_ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def extract_noun_phrases(doc):\n",
    "    \"\"\"Extract noun phrases which are likely to be skills\"\"\"\n",
    "    return [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "\n",
    "def extract_entities(doc):\n",
    "    \"\"\"Extract named entities which might represent technical skills\"\"\"\n",
    "    return [ent.text.lower() for ent in doc.ents]\n",
    "\n",
    "def sliding_window_match_spacy(doc, skill_dictionaries, window_size=6):\n",
    "    \"\"\"\n",
    "    Extract skills where the words appear within a sliding window\n",
    "    Using spaCy's token attributes for better matching\n",
    "    \"\"\"\n",
    "    extracted_skills = defaultdict(set)\n",
    "    \n",
    "    # Convert tokens to text for window matching\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if not token.is_punct and not token.is_space]\n",
    "    \n",
    "    for category, skills in skill_dictionaries.items():\n",
    "        for skill in skills:\n",
    "            skill_words = skill.split()\n",
    "            if len(skill_words) <= 1:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(tokens) - window_size + 1):\n",
    "                window = tokens[i:i+window_size]\n",
    "                \n",
    "                if all(word in window for word in skill_words):\n",
    "                    try:\n",
    "                        positions = [window.index(word) for word in skill_words]\n",
    "                        scatter_factor = max(positions) - min(positions) + 1\n",
    "                        \n",
    "                        if scatter_factor <= len(skill_words) + 2:\n",
    "                            clean_skill = skill.replace('.', '')\n",
    "                            extracted_skills[category].add(clean_skill)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    return extracted_skills\n",
    "\n",
    "def create_spacy_patterns(skill_dictionaries):\n",
    "    \"\"\"Create patterns for spaCy's Matcher\"\"\"\n",
    "    patterns = []\n",
    "    for category, skills in skill_dictionaries.items():\n",
    "        for skill in skills:\n",
    "            # Create pattern for exact match\n",
    "            pattern = [{\"LOWER\": word} for word in skill.split()]\n",
    "            patterns.append((skill, pattern))\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def match_skills_spacy(text):\n",
    "    \"\"\"\n",
    "    Match skills in text using spaCy's capabilities\n",
    "    \"\"\"\n",
    "    extracted_skills = defaultdict(set)\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = preprocess_text_spacy(text)\n",
    "    \n",
    "    # Extract n-grams\n",
    "    ngrams = extract_ngrams_spacy(doc)\n",
    "    \n",
    "    # Extract noun phrases (potential skills)\n",
    "    noun_phrases = extract_noun_phrases(doc)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = extract_entities(doc)\n",
    "    \n",
    "    # Combine all potential skill texts to check against dictionaries\n",
    "    all_candidates = set(ngrams + noun_phrases + entities)\n",
    "    \n",
    "    # Custom regex patterns for specific technical terms\n",
    "    js_frameworks = re.findall(r'\\b\\w+\\.js\\b', text.lower())\n",
    "    sql_db = re.findall(r\"\\b((?:\\w+)ql(?:\\w+)?|(?:\\w+)db)\\b\", text.lower())\n",
    "    \n",
    "    # Match against dictionaries\n",
    "    for category, skills in skill_dictionaries.items():\n",
    "        for skill in skills:\n",
    "            # Direct match for single-word skills\n",
    "            if ' ' not in skill and skill in all_candidates:\n",
    "                clean_skill = skill.replace('.', '')\n",
    "                extracted_skills[category].add(clean_skill)\n",
    "            else:\n",
    "                # For multi-word skills\n",
    "                skill_lower = skill.lower()\n",
    "                for candidate in all_candidates:\n",
    "                    if skill_lower == candidate.lower():\n",
    "                        clean_skill = skill.replace('.', '')\n",
    "                        extracted_skills[category].add(clean_skill)\n",
    "                        break\n",
    "                    # Check if skill is contained within a longer phrase\n",
    "                    elif len(candidate.split()) > len(skill.split()) and skill_lower in candidate.lower():\n",
    "                        candidate_words = candidate.lower().split()\n",
    "                        skill_words = skill_lower.split()\n",
    "                        for i in range(len(candidate_words) - len(skill_words) + 1):\n",
    "                            if ' '.join(candidate_words[i:i+len(skill_words)]) == skill_lower:\n",
    "                                clean_skill = skill.replace('.', '')\n",
    "                                extracted_skills[category].add(clean_skill)\n",
    "                                break\n",
    "    \n",
    "    # Add sliding window matches\n",
    "    sliding_matches = sliding_window_match_spacy(doc, skill_dictionaries)\n",
    "    for category, skills in sliding_matches.items():\n",
    "        extracted_skills[category].update(skills)\n",
    "    \n",
    "    # Add specialized matches from regex\n",
    "    for skill in js_frameworks:\n",
    "        clean_skill = skill.replace('.', '')\n",
    "        extracted_skills[\"Framework (FW)\"].add(clean_skill)\n",
    "    \n",
    "    for skill in sql_db:\n",
    "        clean_skill = skill.replace('.', '')\n",
    "        extracted_skills[\"Database (DB)\"].add(clean_skill)\n",
    "    \n",
    "    # Add lemma-based matching for variations of the same word\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.lower()\n",
    "        # Check if the lemma is in any skill list\n",
    "        for category, skills in skill_dictionaries.items():\n",
    "            for skill in skills:\n",
    "                if ' ' not in skill and lemma == skill:\n",
    "                    clean_skill = skill.replace('.', '')\n",
    "                    extracted_skills[category].add(clean_skill)\n",
    "    \n",
    "    return extracted_skills\n",
    "\n",
    "def word2features(sent, i):\n",
    "    \"\"\"Extract features for a given word in a sentence\"\"\"\n",
    "    word = sent[i][0]  # The word itself\n",
    "    \n",
    "    # Basic features\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],  # Suffix\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "\n",
    "# Features for words at the beginning of sentence\n",
    "    if i == 0:\n",
    "        features['BOS'] = True\n",
    "    else:\n",
    "        word_prev = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word_prev.lower(),\n",
    "            '-1:word.istitle()': word_prev.istitle(),\n",
    "            '-1:word.isupper()': word_prev.isupper(),\n",
    "        })\n",
    "    \n",
    "    # Features for words at the end of sentence\n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True\n",
    "    else:\n",
    "        word_next = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word_next.lower(),\n",
    "            '+1:word.istitle()': word_next.istitle(),\n",
    "            '+1:word.isupper()': word_next.isupper(),\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"Extract features for all words in a sentence\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\"Extract labels for all words in a sentence\"\"\"\n",
    "    return [token[1] for token in sent]\n",
    "\n",
    "crf = joblib.load('crf_model.joblib')\n",
    "def predict_entities(text):\n",
    "    # Tokenize the text (you might want to use your existing tokenization)\n",
    "    tokens = text.split()  # Simple splitting, you might want something more sophisticated\n",
    "    # Create features\n",
    "    features = sent2features([(token, 'O') for token in tokens])  # Dummy labels\n",
    "    \n",
    "    # Predict\n",
    "    predictions = crf.predict([features])[0]\n",
    "    \n",
    "    # Combine tokens with their predictions + generate dictionary format\n",
    "    predictions_dict = generate_dictionary(list(zip(tokens, predictions)))\n",
    "    \n",
    "    return predictions_dict\n",
    "\n",
    "def generate_dictionary(prediction_list):\n",
    "    all_labels = {\n",
    "        \"PL\": \"Programming Languages\",\n",
    "        \"FW\": \"Frameworks\",\n",
    "        \"DB\": \"Databases\",\n",
    "        \"CP\": \"Cloud Platforms\",\n",
    "        \"DO\": \"DevOps\",\n",
    "        \"NS\": \"Network & Security\",\n",
    "        \"DAS\": \"Data Analysis & Science\",\n",
    "        \"SWE\": \"Software Engineering\",\n",
    "        \"PM\": \"Project Management\",\n",
    "        \"EC\": \"Education Certification\",\n",
    "        \"SS\": \"Soft Skills\",\n",
    "        \"O\": \"Outside\"\n",
    "    }\n",
    "    \n",
    "    prediction_dict = {label: [] for label in all_labels}\n",
    "    current_phrase = []\n",
    "    current_label = None\n",
    "    \n",
    "    for word, label in prediction_list:\n",
    "        if label == \"O\":\n",
    "            if current_phrase and current_label:\n",
    "                prediction_dict[current_label].append(\" \".join(current_phrase))\n",
    "                current_phrase = []\n",
    "                current_label = None\n",
    "            prediction_dict[\"O\"].append(word)\n",
    "        else:\n",
    "            main_category = label[2:]\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_phrase and current_label:\n",
    "                    prediction_dict[current_label].append(\" \".join(current_phrase))\n",
    "                current_phrase = [word]\n",
    "                current_label = main_category\n",
    "            elif label.startswith(\"I-\") and current_label == main_category:\n",
    "                current_phrase.append(word)\n",
    "    \n",
    "    if current_phrase and current_label:\n",
    "        prediction_dict[current_label].append(\" \".join(current_phrase))\n",
    "    \n",
    "    normalized_dict = {\n",
    "        \"Programming Languages (PL)\": [],\n",
    "        \"Frameworks (FW)\": [],\n",
    "        \"Databases (DB)\": [],\n",
    "        \"Cloud Platforms (CP)\": [],\n",
    "        \"DevOps (DO)\": [],\n",
    "        \"Network & Security (NS)\": [],\n",
    "        \"Data Analysis & Science (DAS)\": [],\n",
    "        \"Software Engineering (SWE)\": [],\n",
    "        \"Project Management (PM)\": [],\n",
    "        \"Education Certification (EC)\": [],\n",
    "        \"Soft Skills (SS)\": [],\n",
    "        \"Outside (O)\": []\n",
    "    }\n",
    "\n",
    "    for key in prediction_dict.keys():\n",
    "        new_key = all_labels[key] + \" (\" + key + \")\"\n",
    "        normalized_dict[new_key] = prediction_dict[key]\n",
    "    \n",
    "    return normalized_dict\n",
    "\n",
    "def text2vec(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def find_similar_texts(input, corpus, model):\n",
    "    input_vector = text2vec(input, model)\n",
    "    if input_vector is None:\n",
    "        return \"Could not generate vector for input text\"\n",
    "    \n",
    "    similarities = []\n",
    "    for jobID, text in corpus:\n",
    "        text_vector = text2vec(text, model)\n",
    "\n",
    "        if text_vector is not None:\n",
    "            similarity = cosine_similarity([input_vector], [text_vector])[0][0]\n",
    "            similarities.append((jobID, similarity.item()*100))\n",
    "        else:\n",
    "            similarities.append((jobID, 0))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "matching_model =  Word2Vec.load(\"matching.model\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f383733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Using cached sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
      "  Using cached python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in ./.venv/lib/python3.11/site-packages (from sklearn_crfsuite) (1.6.1)\n",
      "Collecting tabulate>=0.4.2 (from sklearn_crfsuite)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in ./.venv/lib/python3.11/site-packages (from sklearn_crfsuite) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.6.0)\n",
      "Using cached sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl (319 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn_crfsuite\n",
      "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f0d10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    \"\"\"Extract text from a PDF file directly from file stream using PyMuPDF.\"\"\"\n",
    "    with open(file, \"rb\") as f:\n",
    "        pdf_data = f.read()\n",
    "    doc = pymupdf.open(stream=io.BytesIO(pdf_data), filetype=\"pdf\")  # Open PDF from stream\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "def combineSkillsToString(skills_json):\n",
    "    finalString = \"\"\n",
    "    for category, skillList in skills_json.items():\n",
    "        if len(skillList) > 0:\n",
    "            finalString += ((\" \".join(skillList)) + \" \")\n",
    "    return finalString\n",
    "\n",
    "skillsList = []\n",
    "\n",
    "resumeText = extract_text_from_pdf(Path(\"./yr_resume.pdf\"))\n",
    "resumeSkills = match_skills_spacy(resumeText)\n",
    "education_skills = resumeSkills[\"Education Certification (EC)\"]\n",
    "predictions = predict_entities(resumeText)\n",
    "outside_words = predictions[\"Outside (O)\"]  # Extract \"Outside\" words/phrases from Model 2\n",
    "skills_from_outside = match_skills_spacy(\" \".join(outside_words))\n",
    "\n",
    "for category, skills in predictions.items():\n",
    "    if category != \"Outside (O)\" and category != \"Education Certification (EC)\":\n",
    "        for word in skills:\n",
    "            skills_from_outside[category].add(preprocess_text(word))\n",
    "\n",
    "skills_from_outside[\"Education Certification (EC)\"] = education_skills\n",
    "outside_skills_dict = {key:list(itemList) for key,itemList in skills_from_outside.items()}\n",
    "skillsList.append((\"user\",combineSkillsToString(outside_skills_dict)))\n",
    "\n",
    "\n",
    "with open(\"./extracted_job_skills.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            id = row[\"job_id\"]\n",
    "            skills = row[\"extracted_skills\"]\n",
    "            combinedSkills = combineSkillsToString(json.loads(skills))\n",
    "            skillsList.append((id,combinedSkills))\n",
    "        file.close()\n",
    "\n",
    "df = pd.DataFrame(columns=[\"jobId/user\", \"skillString\"],data=skillsList)\n",
    "df.to_csv(\"./combined_skills.csv\",index=False)\n",
    "\n",
    "matchingScores = find_similar_texts(skillsList[0][1], skillsList[1:], matching_model)\n",
    "match_df = pd.DataFrame(columns=[\"jobId/user\", \"match_score\"], data=matchingScores)\n",
    "match_df.to_csv(\"./match_scores.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
