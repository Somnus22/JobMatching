{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy\n",
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "df = pd.read_csv('combined_resume_data.csv')\n",
    "totalRows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and remove stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#resume_tokens = word_tokenize(resume_skills.lower())\n",
    "\n",
    "# convert to set to remove duplicates then back to list\n",
    "#unique_tokens = list(set(resume_tokens))\n",
    "\n",
    "#filtered_tokens = [word for word in resume_tokens if word.isalpha() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from collections import defaultdict\n",
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stop words from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text string\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with stop words removed\n",
    "    \"\"\"\n",
    "    # Download stopwords if not already downloaded\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "        \n",
    "    # Download punkt tokenizer if not already downloaded\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Get English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in word_tokens if word not in stop_words and word.isalnum()]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTED ENTITIES ===\n",
      "\n",
      "Programming Language (PL): javascript, python, sql\n",
      "Framework (FW): flask\n",
      "Database (DB): mysql\n",
      "Education Certification (EC): be\n",
      "Soft Skills (SS): communication\n",
      "\n",
      "=== LEFTOVER TEXT ===\n",
      "\n",
      "web systems developer tech support lead junior it technician c git html     bash css web systems developer web systems developer web systems developer atc communications arapahoe ne work experience web systems developer atc communications 2018 to present work with my manager to upgrade systems pppoe server dhcp server network managers documentation site and more write  and  scripts to automate  between those systems tech support lead atc communications 2016 to 2018 help customers over the phone train new techs build atcjet net website and techassist web app from scratch junior it technician data com solutions 2013 to 2015 support homes and businesses for general it problems and solutions develop in house tools for automating work like downloading updates and backing up servers education high school or equivalent in computer science thomas edison university 2017 skills c git html     bash css links http linkedin com in spideyclick http github com spideyclick http youtu  la1jko nu8 http spideyclick net additional information skills html css bash  git    learning webgl and c i work with lots of centos 7 virtual servers right now\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Expanded regex patterns for better matching\n",
    "patterns = {\n",
    "    \"Programming Language (PL)\": r\"\\b(?:python\\d*|java(?:script)?|c\\+\\+|c#|sql|ruby|go|swift|typescript|r|kotlin)\\b\",\n",
    "    \n",
    "    \"Framework (FW)\": r\"\\b(?:django|flask|spring|react(?:.js)?|angular|vue|express|fastapi|\\.net|laravel)\\b\",\n",
    "    \n",
    "    \"Database (DB)\": r\"\\b(?:sql\\s*server|mysql|postgresql|mongodb|oracle|sqlite|firebase|cassandra|database(?:\\s+\\w+){0,1})\\b\",\n",
    "    \n",
    "    \"Cloud Platform (CP)\": r\"\\b(?:aws(?:\\s+\\w+){0,2}|amazon\\s*web\\s*services|azure|google\\s*cloud|gcp|ibm\\s*cloud|digitalocean|heroku)\\b\",\n",
    "    \n",
    "    \"DevOps (DO)\": r\"\\b(?:docker(?:ized)?|kubernetes|jenkins|terraform|ansible|ci/cd|travis\\s*ci|circleci)\\b\",\n",
    "    \n",
    "    \"Network & Security\": r\"\\b(?:firewall|vpn|ssl/tls|penetration\\s*testing|ids|ips|tcp/ip|zero\\s*trust)\\b\",\n",
    "    \n",
    "    \"Data Analysis & Science\": r\"\\b(?:pandas|numpy|scikit-learn|tensorflow|power\\s*bi|excel|tableau|matplotlib|data\\s*visualisation|data\\s*visualization|visualizing\\s*data|AI|artificial\\s*intelligence|machine\\s*learning|ML|NLP|natural\\s*language\\s*processing|text\\s*analytics|language\\s*model(?:s|ing)?|transaction\\s*management|data\\s*transaction(?:s)?)\\b\",\n",
    "    \n",
    "    \"Software Engineering (SWE)\": r\"\\b(?:software\\s*development|design\\s*patterns|unit\\s*testing|full\\s*stack|fullstack|full-stack|software\\s*engineer(?:ing)?|junior\\s*(?:software\\s*)?engineer|senior\\s*(?:software\\s*)?engineer|staff\\s*engineer|principal\\s*engineer|software\\s*engineer\\s*intern(?:ship)?|code\\s*optimi(?:s|z)ation|performance\\s*tuning|code\\s*refactoring|refactor(?:ing)?|microservice(?:s)?|MSA|microservice\\s*architecture|containerization|docker|kubernetes|k8s|code\\s*review(?:s)?|peer\\s*review(?:s)?|eclipse|eclipse\\s*ide|hibernate|hibernate\\s*orm|jquery|rest(?:ful)?\\s*api(?:s)?|rest\\s*api(?:s)?|restful\\s*web\\s*service(?:s)?|api\\s*development|web\\s*service(?:s)?|object\\s*oriented\\s*programming|oop|object\\s*oriented\\s*design|jsp|java\\s*server\\s*pages|rpc|remote\\s*procedure\\s*call|j2ee|java\\s*ee|jvm|java\\s*virtual\\s*machine|jax|jax-rs|jax-ws|apache|apache\\s*(?:kafka|tomcat|maven|ant|struts|camel|spark|hadoop|flink)|bootstrap|front-end\\s*framework|rabbitmq|message\\s*(?:queue|broker)|front[\\s-]end|back[\\s-]end|web\\s*design|ui|user\\s*interface|ux|user\\s*experience|ui\\/ux|front[\\s-]end\\s*development|back[\\s-]end\\s*development|web\\s*development)\\b\",\n",
    "    \n",
    "    \"Project Management (PM)\": r\"\\b(?:agile|jira|trello|asana|kanban|prince2|stakeholder\\s*management|(?<!certified\\s)scrum(?!\\smaster))\\b\",\n",
    "\n",
    "    \"Education Certification (EC)\": r\"\\bcertified\\s+scrum\\s+master\\b|\\bcsm\\b|\\bpmp\\b|\\baws\\s+certified\\b|\\bazure\\s+certified\\b|\\bgcp\\s+certified\\b|\\bcissp\\b|\\bccna\\b|\\bceh\\b|\\bcomptia\\b|\\bcisa\\b|\\bcism\\b|\\b(?:bachelor(?:\\s+of\\s+(?:science|engineering|computer\\s+science|information\\s+technology|information\\s+systems|cybersecurity|data\\s+science|software\\s+engineering))?(?:\\s+in\\s+(?:computer\\s+science|computer\\s+engineering|information\\s+technology|information\\s+systems|cybersecurity|data\\s+science|software\\s+engineering|artificial\\s+intelligence|machine\\s+learning))?|B\\.?S\\.?|B\\.?E\\.?|B\\.?C\\.?S\\.?|B\\.?Tech\\.?|(?<!certified\\s)master(?:\\s+of\\s+(?:science|engineering|computer\\s+science|information\\s+technology|information\\s+systems|cybersecurity|data\\s+science|software\\s+engineering))?(?:\\s+in\\s+(?:computer\\s+science|computer\\s+engineering|information\\s+technology|information\\s+systems|cybersecurity|data\\s+science|software\\s+engineering|artificial\\s+intelligence|machine\\s+learning))?|M\\.?S\\.?|M\\.?E\\.?|M\\.?C\\.?S\\.?|M\\.?Tech\\.?|phd|ph\\.?d\\.?|doctorate|doctor\\s+of\\s+philosophy(?:\\s+in\\s+(?:computer\\s+science|computer\\s+engineering|information\\s+technology|data\\s+science))?|specialisation\\s+in\\s+(?:software|data|cloud|security|networking|ai|machine\\s+learning)|specialization\\s+in\\s+(?:software|data|cloud|security|networking|ai|machine\\s+learning)|minor\\s+in\\s+(?:computer\\s+science|information\\s+technology|data\\s+science|software\\s+engineering)|major\\s+in\\s+(?:computer\\s+science|information\\s+technology|data\\s+science|software\\s+engineering))\\b\",\n",
    "    \n",
    "    \"Soft Skills (SS)\": r\"\\b(?:communication|leadership|teamwork|problem\\s*solving|critical\\s*thinking|adaptability)\\b\"\n",
    "}\n",
    "\n",
    "test_text = df.iloc[random.randint(1, totalRows)]['clean_text']\n",
    "\n",
    "extracted_entities = defaultdict(set)\n",
    "#modified_text = remove_stopwords(test_text_2)\n",
    "modified_text = test_text\n",
    "\n",
    "# Apply regex patterns with wildcard matching\n",
    "for label, pattern in patterns.items():\n",
    "    # Define the replacement function inside the loop\n",
    "    # This creates a new function for each iteration\n",
    "    def replacement_func(match_obj):\n",
    "        match_text = match_obj.group(0)\n",
    "        \n",
    "        # Add the match to extracted entities\n",
    "        if isinstance(match_text, tuple):  # Handle tuple case from capture groups\n",
    "            extracted_entities[label].update(map(str.lower, match_text))\n",
    "        else:\n",
    "            extracted_entities[label].add(match_text.lower())\n",
    "            \n",
    "        # Return empty string to remove the match from the text\n",
    "        return \"\"\n",
    "    \n",
    "    # Find matches and replace them with empty string\n",
    "    modified_text = re.sub(pattern, replacement_func, modified_text, flags=re.IGNORECASE)\n",
    "\n",
    "# Print the extracted entities\n",
    "print(\"=== EXTRACTED ENTITIES ===\\n\")\n",
    "for label, entities in extracted_entities.items():\n",
    "    # Convert set to a comma-separated string for printing\n",
    "    entities_str = \", \".join(sorted(entities))\n",
    "    print(f\"{label}: {entities_str}\")\n",
    "\n",
    "# Print the leftover text\n",
    "print(\"\\n=== LEFTOVER TEXT ===\\n\")\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3, sql, database administration, relational database management, aws amplify, aws cloud, aws ec2, aws snowflake, dockerized, agil\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "test = stemmer.stem(\"python3, sql, database administration, relational database management, aws amplify, aws cloud, aws ec2, aws snowflake, dockerized, agile\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database administration aws\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "test2 = lemmatizer.lemmatize(\"database administration aws\")\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
