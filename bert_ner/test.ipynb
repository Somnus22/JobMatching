{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved model\n",
    "MODEL_PATH = \"./ner_skills_model\"  # Update this to your model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # Merge entities with same type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create NER pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # Merge entities with same type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to extract and organize entities from resume text\n",
    "def extract_resume_entities(resume_text):\n",
    "    print(\"Processing resume text...\")\n",
    "    # Split text into manageable chunks if it's very long\n",
    "    # This helps avoid potential issues with maximum sequence length\n",
    "    max_length = 512\n",
    "    chunks = []\n",
    "    \n",
    "    if len(resume_text) > max_length:\n",
    "        # Simple chunking by sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', resume_text)\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < max_length:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "    else:\n",
    "        chunks = [resume_text]\n",
    "    \n",
    "    # Process each chunk\n",
    "    all_entities = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        entities = ner_pipeline(chunk)\n",
    "        all_entities.extend(entities)\n",
    "    \n",
    "    # Organize entities by category\n",
    "    organized_entities = defaultdict(list)\n",
    "    \n",
    "    for entity in all_entities:\n",
    "        entity_text = entity[\"word\"]\n",
    "        entity_type = entity[\"entity_group\"]\n",
    "        score = round(entity[\"score\"], 3)\n",
    "        \n",
    "        # Add to appropriate category with confidence score\n",
    "        organized_entities[entity_type].append({\n",
    "            \"text\": entity_text,\n",
    "            \"confidence\": score\n",
    "        })\n",
    "    \n",
    "    return organized_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process your resume!\n",
      "Successfully read resume from test.txt\n"
     ]
    }
   ],
   "source": [
    "# 4. Test with your resume\n",
    "print(\"Ready to process your resume!\")\n",
    "\n",
    "# Option 1: Read from a file\n",
    "resume_file_path = \"test.txt\"  # Update this path\n",
    "try:\n",
    "    with open(resume_file_path, 'r', encoding='utf-8') as file:\n",
    "        resume_text = file.read()\n",
    "    print(f\"Successfully read resume from {resume_file_path}\")\n",
    "except:\n",
    "    print(f\"Could not read from {resume_file_path}\")\n",
    "    # Fallback to manual input\n",
    "    resume_text = \"\"\"\n",
    "    # Paste your resume text here if reading from file fails\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resume text...\n",
      "Processing chunk 1/13...\n",
      "Processing chunk 2/13...\n",
      "Processing chunk 3/13...\n",
      "Processing chunk 4/13...\n",
      "Processing chunk 5/13...\n",
      "Processing chunk 6/13...\n",
      "Processing chunk 7/13...\n",
      "Processing chunk 8/13...\n",
      "Processing chunk 9/13...\n",
      "Processing chunk 10/13...\n",
      "Processing chunk 11/13...\n",
      "Processing chunk 12/13...\n",
      "Processing chunk 13/13...\n",
      "\n",
      "=== EXTRACTED INFORMATION FROM YOUR RESUME ===\n",
      "\n",
      "\n",
      "MISC:\n",
      "  - Git (confidence: 0.8650000095367432)\n",
      "  - ##S (confidence: 0.7459999918937683)\n",
      "  - ##ven (confidence: 0.7170000076293945)\n",
      "  - S3 (confidence: 0.6980000138282776)\n",
      "  - CI (confidence: 0.6890000104904175)\n",
      "  - CSV (confidence: 0.6710000038146973)\n",
      "  - ESLint (confidence: 0.6669999957084656)\n",
      "  - AMQP (confidence: 0.652999997138977)\n",
      "  - ##Z (confidence: 0.6470000147819519)\n",
      "  - WAF (confidence: 0.6150000095367432)\n",
      "  - NextJS (confidence: 0.5899999737739563)\n",
      "  - ##DS (confidence: 0.5879999995231628)\n",
      "  - ##au (confidence: 0.5479999780654907)\n",
      "  - EC2 (confidence: 0.5479999780654907)\n",
      "  - GitHub (confidence: 0.546999990940094)\n",
      "  - Shadcn (confidence: 0.5429999828338623)\n",
      "  - RDS (confidence: 0.5249999761581421)\n",
      "  - ##ub (confidence: 0.4909999966621399)\n",
      "  - ##ps (confidence: 0.49000000953674316)\n",
      "  - ##O (confidence: 0.4880000054836273)\n",
      "  - Pandas (confidence: 0.4830000102519989)\n",
      "  - ##p (confidence: 0.47999998927116394)\n",
      "  - ##press (confidence: 0.4779999852180481)\n",
      "  - GoTest (confidence: 0.47099998593330383)\n",
      "  - ##ra (confidence: 0.4659999907016754)\n",
      "  - Strip (confidence: 0.4569999873638153)\n",
      "  - ##ithub (confidence: 0.4480000138282776)\n",
      "  - Go (confidence: 0.4339999854564667)\n",
      "  - Kafka (confidence: 0.4309999942779541)\n",
      "  - ##TP (confidence: 0.4180000126361847)\n",
      "  - ##ue (confidence: 0.41200000047683716)\n",
      "  - - (confidence: 0.39399999380111694)\n",
      "  - ##TL (confidence: 0.39100000262260437)\n",
      "  - ##lang (confidence: 0.38600000739097595)\n",
      "  - Rabbit (confidence: 0.36800000071525574)\n",
      "\n",
      "PL:\n",
      "  - Python (confidence: 0.9760000109672546)\n",
      "  - Java (confidence: 0.9729999899864197)\n",
      "  - Kubernetes (confidence: 0.8370000123977661)\n",
      "  - TypeScript (confidence: 0.8209999799728394)\n",
      "  - Firebase (confidence: 0.8199999928474426)\n",
      "  - CloudWatch monitoring (confidence: 0.8140000104904175)\n",
      "  - Docker (confidence: 0.7990000247955322)\n",
      "  - Actions (confidence: 0.7950000166893005)\n",
      "  - Google Maps (confidence: 0.7749999761581421)\n",
      "  - RabbitMQ (confidence: 0.7730000019073486)\n",
      "  - Spring Boot Cloud (confidence: 0.7680000066757202)\n",
      "  - React (confidence: 0.765999972820282)\n",
      "  - RESTful API (confidence: 0.7639999985694885)\n",
      "  - Cloud Engineering (confidence: 0.7620000243186951)\n",
      "  - task management (confidence: 0.7549999952316284)\n",
      "  - ##House (confidence: 0.7419999837875366)\n",
      "  - ##MQ (confidence: 0.7409999966621399)\n",
      "  - Clean Architecture (confidence: 0.734000027179718)\n",
      "  - Grafana Databases (confidence: 0.734000027179718)\n",
      "  - Enterprise Solution Development (confidence: 0.7260000109672546)\n",
      "  - ClickHouse (confidence: 0.7250000238418579)\n",
      "  - machine learning (confidence: 0.7229999899864197)\n",
      "  - ##ci (confidence: 0.7210000157356262)\n",
      "  - Amazon CloudFront (confidence: 0.7089999914169312)\n",
      "  - ##ing (confidence: 0.7070000171661377)\n",
      "  - Grafana monitoring (confidence: 0.7059999704360962)\n",
      "  - Filebeat (confidence: 0.7039999961853027)\n",
      "  - secure (confidence: 0.703000009059906)\n",
      "  - ##e (confidence: 0.6990000009536743)\n",
      "  - Discord (confidence: 0.6919999718666077)\n",
      "  - pro (confidence: 0.6869999766349792)\n",
      "  - Software (confidence: 0.6779999732971191)\n",
      "  - Data Analytics (confidence: 0.675000011920929)\n",
      "  - Grafana payment monitoring (confidence: 0.6740000247955322)\n",
      "  - Vercel (confidence: 0.6700000166893005)\n",
      "  - CleanShift (confidence: 0.6610000133514404)\n",
      "  - Prometheus (confidence: 0.6610000133514404)\n",
      "  - Oracle Java (confidence: 0.6589999794960022)\n",
      "  - static content delivery (confidence: 0.652999997138977)\n",
      "  - DevO (confidence: 0.6460000276565552)\n",
      "  - Microservices (confidence: 0.6449999809265137)\n",
      "  - Cy (confidence: 0.6439999938011169)\n",
      "  - Cypress Core (confidence: 0.6439999938011169)\n",
      "  - ClickHouse T (confidence: 0.6420000195503235)\n",
      "  - Performance Optimization (confidence: 0.6420000195503235)\n",
      "  - feature engineering (confidence: 0.6349999904632568)\n",
      "  - order creation (confidence: 0.6269999742507935)\n",
      "  - dashboard (confidence: 0.6240000128746033)\n",
      "  - Geocoding (confidence: 0.6200000047683716)\n",
      "  - Ji (confidence: 0.6169999837875366)\n",
      "  - Hugging Face transform (confidence: 0.6079999804496765)\n",
      "  - architecture (confidence: 0.6000000238418579)\n",
      "  - BeautifulSou (confidence: 0.6000000238418579)\n",
      "  - parallel queue processing (confidence: 0.6000000238418579)\n",
      "  - Spring Boot (confidence: 0.597000002861023)\n",
      "  - reporting (confidence: 0.597000002861023)\n",
      "  - Firestore (confidence: 0.5960000157356262)\n",
      "  - search (confidence: 0.5929999947547913)\n",
      "  - Stripe checkout (confidence: 0.5929999947547913)\n",
      "  - system health monitoring (confidence: 0.5879999995231628)\n",
      "  - user management (confidence: 0.5870000123977661)\n",
      "  - ##onous (confidence: 0.5830000042915344)\n",
      "  - ExpressJ (confidence: 0.5820000171661377)\n",
      "  - Tableau (confidence: 0.5789999961853027)\n",
      "  - Maintenance (confidence: 0.578000009059906)\n",
      "  - Cronjobs (confidence: 0.5759999752044678)\n",
      "  - Logstash (confidence: 0.574999988079071)\n",
      "  - Golang Echo (confidence: 0.574999988079071)\n",
      "  - A (confidence: 0.574999988079071)\n",
      "  - automated backup (confidence: 0.5740000009536743)\n",
      "  - NextJ (confidence: 0.5720000267028809)\n",
      "  - identity federation (confidence: 0.5619999766349792)\n",
      "  - data replication (confidence: 0.5619999766349792)\n",
      "  - ##ive design (confidence: 0.5580000281333923)\n",
      "  - On (confidence: 0.5519999861717224)\n",
      "  - error handling (confidence: 0.550000011920929)\n",
      "  - C (confidence: 0.5460000038146973)\n",
      "  - Vuetify (confidence: 0.5460000038146973)\n",
      "  - service (confidence: 0.5450000166893005)\n",
      "  - Flask (confidence: 0.5440000295639038)\n",
      "  - ##l (confidence: 0.5410000085830688)\n",
      "  - re (confidence: 0.5400000214576721)\n",
      "  - ##in (confidence: 0.5360000133514404)\n",
      "  - ##ytics (confidence: 0.5239999890327454)\n",
      "  - Vue (confidence: 0.5239999890327454)\n",
      "  - control (confidence: 0.5199999809265137)\n",
      "  - ##M (confidence: 0.5199999809265137)\n",
      "  - ##ly (confidence: 0.5189999938011169)\n",
      "  - notification (confidence: 0.515999972820282)\n",
      "  - migration (confidence: 0.5130000114440918)\n",
      "  - assignment (confidence: 0.5130000114440918)\n",
      "  - ##ana (confidence: 0.5099999904632568)\n",
      "  - ##aging (confidence: 0.5090000033378601)\n",
      "  - ##cro (confidence: 0.5080000162124634)\n",
      "  - Bash (confidence: 0.5059999823570251)\n",
      "  - Go (confidence: 0.5)\n",
      "  - J (confidence: 0.5)\n",
      "  - management (confidence: 0.49900001287460327)\n",
      "  - ##mLets (confidence: 0.49900001287460327)\n",
      "  - analysis (confidence: 0.49799999594688416)\n",
      "  - ##H (confidence: 0.49300000071525574)\n",
      "  - ##up (confidence: 0.49000000953674316)\n",
      "  - ##ness (confidence: 0.4860000014305115)\n",
      "  - Ma (confidence: 0.4819999933242798)\n",
      "  - development (confidence: 0.4819999933242798)\n",
      "  - smooth (confidence: 0.47999998927116394)\n",
      "  - oriented (confidence: 0.4749999940395355)\n",
      "  - Cale (confidence: 0.4729999899864197)\n",
      "  - code (confidence: 0.47200000286102295)\n",
      "  - processing (confidence: 0.47200000286102295)\n",
      "  - ##Hub (confidence: 0.46399998664855957)\n",
      "  - Gmail SM (confidence: 0.460999995470047)\n",
      "  - not (confidence: 0.46000000834465027)\n",
      "  - ##chema (confidence: 0.45899999141693115)\n",
      "  - ##rtifications (confidence: 0.45899999141693115)\n",
      "  - ##met (confidence: 0.4359999895095825)\n",
      "  - Flask Python (confidence: 0.43299999833106995)\n",
      "  - behavior analyses (confidence: 0.4309999942779541)\n",
      "  - R (confidence: 0.42500001192092896)\n",
      "  - V (confidence: 0.3499999940395355)\n",
      "\n",
      "DB:\n",
      "  - MySQL (confidence: 0.9330000281333923)\n",
      "  - SQL (confidence: 0.9240000247955322)\n",
      "  - PostgreSQL (confidence: 0.7360000014305115)\n",
      "  - AWS (confidence: 0.5410000085830688)\n",
      "  - A (confidence: 0.4339999854564667)\n",
      "  - Kafka (confidence: 0.3580000102519989)\n",
      "  - Ka (confidence: 0.2709999978542328)\n",
      "  - ##ka (confidence: 0.24899999797344208)\n",
      "\n",
      "=== ANALYSIS COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "# Process the resume\n",
    "entities = extract_resume_entities(resume_text)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== EXTRACTED INFORMATION FROM YOUR RESUME ===\\n\")\n",
    "\n",
    "if not entities:\n",
    "    print(\"No entities were detected. This might indicate:\")\n",
    "    print(\"1. The model needs more training\")\n",
    "    print(\"2. The resume format is very different from training data\")\n",
    "    print(\"3. There might be an issue with how the text is being processed\")\n",
    "else:\n",
    "    # Print each entity type and its found entities\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        print(f\"\\n{entity_type.upper()}:\")\n",
    "        # Sort by confidence score\n",
    "        sorted_entities = sorted(entity_list, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in sorted_entities:\n",
    "            if entity[\"text\"].lower() not in seen:\n",
    "                seen.add(entity[\"text\"].lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        # Print unique entities with their confidence scores\n",
    "        for entity in unique_entities:\n",
    "            print(f\"  - {entity['text']} (confidence: {entity['confidence']})\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Kafka', 'confidence': 0.358},\n",
       " {'text': 'Ka', 'confidence': 0.271},\n",
       " {'text': '##ka', 'confidence': 0.249},\n",
       " {'text': 'AWS', 'confidence': 0.509},\n",
       " {'text': 'PostgreSQL', 'confidence': 0.736},\n",
       " {'text': 'AWS', 'confidence': 0.541},\n",
       " {'text': 'AWS', 'confidence': 0.376},\n",
       " {'text': 'A', 'confidence': 0.434},\n",
       " {'text': 'AWS', 'confidence': 0.418},\n",
       " {'text': 'AWS', 'confidence': 0.386},\n",
       " {'text': 'AWS', 'confidence': 0.35},\n",
       " {'text': 'SQL', 'confidence': 0.924},\n",
       " {'text': 'AWS', 'confidence': 0.398},\n",
       " {'text': 'PostgreSQL', 'confidence': 0.712},\n",
       " {'text': 'MySQL', 'confidence': 0.933}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities['DB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
